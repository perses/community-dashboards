groups:
- name: thanos-component-absent
  rules:
  - alert: ThanosCompactIsDown
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanoscompact
      description: ThanosCompact has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      message: ThanosCompact has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanoscompactisdown
      summary: Thanos component has disappeared from {{$labels.namespace}}.
    expr: absent(up{job=~"thanos-compact.*"} == 1)
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosQueryIsDown
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosquery
      description: ThanosQuery has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      message: ThanosQuery has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosqueryisdown
      summary: Thanos component has disappeared from {{$labels.namespace}}.
    expr: absent(up{job=~"thanos-query.*"} == 1)
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosReceiveRouterIsDown
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: ThanosReceiveRouter has disappeared from {{$labels.namespace}}.
        Prometheus target for the component cannot be discovered.
      message: ThanosReceiveRouter has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceiveisdown
      summary: Thanos component has disappeared from {{$labels.namespace}}.
    expr: absent(up{job=~"thanos-receive-router.*"} == 1)
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosReceiveIngesterIsDown
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: ThanosReceiveIngester has disappeared from {{$labels.namespace}}.
        Prometheus target for the component cannot be discovered.
      message: ThanosReceiveIngester has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceiveisdown
      summary: Thanos component has disappeared from {{$labels.namespace}}.
    expr: absent(up{job=~"thanos-receive-ingester.*"} == 1)
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosRuleIsDown
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: ThanosRule has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      message: ThanosRule has disappeared from {{$labels.namespace}}. Prometheus target
        for the component cannot be discovered.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosruleisdown
      summary: Thanos component has disappeared from {{$labels.namespace}}.
    expr: absent(up{job=~"thanos-ruler.*"} == 1)
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosStoreIsDown
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosstore
      description: ThanosStore has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      message: ThanosStore has disappeared from {{$labels.namespace}}. Prometheus
        target for the component cannot be discovered.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosstoreisdown
      summary: Thanos component has disappeared from {{$labels.namespace}}.
    expr: absent(up{job=~"thanos-store.*"} == 1)
    for: 5m
    labels:
      service: thanos
      severity: high
- name: thanos-compact
  rules:
  - alert: ThanosCompactMultipleRunning
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanoscompact
      description: No more than one Thanos Compact instance should be running at once.
        There are {{$value}} in {{$labels.namespace}} instances running.
      message: No more than one Thanos Compact instance should be running at once.
        There are {{$value}} in {{$labels.namespace}} instances running.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanoscompactmultiplerunning
      summary: Thanos Compact has multiple instances running.
    expr: sum by (namespace, job) (up{job=~"thanos-compact.*"} > 1)
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosCompactHalted
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanoscompact
      description: Thanos Compact {{$labels.job}} in {{$labels.namespace}} has failed
        to run and now is halted.
      message: Thanos Compact {{$labels.job}} in {{$labels.namespace}} has failed
        to run and now is halted.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanoscompacthalted
      summary: Thanos Compact has failed to run and is now halted.
    expr: thanos_compact_halted{job=~"thanos-compact.*"} == 1
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosCompactHighCompactionFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanoscompact
      description: Thanos Compact {{$labels.job}} in {{$labels.namespace}} is failing
        to execute {{$value | humanize}}% of compactions.
      message: Thanos Compact {{$labels.job}} in {{$labels.namespace}} is failing
        to execute {{$value | humanize}}% of compactions.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanoscompacthighcompactionfailures
      summary: Thanos Compact is failing to execute compactions.
    expr: |2-
            sum by (namespace, job) (
              rate(thanos_compact_group_compactions_failures_total{job=~"thanos-compact.*"}[5m])
            )
          /
            sum by (namespace, job) (rate(thanos_compact_group_compactions_total{job=~"thanos-compact.*"}[5m]))
        *
          100
      >
        5
    for: 15m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosCompactBucketHighOperationFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanoscompact
      description: Thanos Compact {{$labels.job}} in {{$labels.namespace}} Bucket
        is failing to execute {{$value | humanize}}% of operations.
      message: Thanos Compact {{$labels.job}} in {{$labels.namespace}} Bucket is failing
        to execute {{$value | humanize}}% of operations.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanoscompactbuckethighoperationfailures
      summary: Thanos Compact Bucket is having a high number of operation failures.
    expr: |2-
            sum by (namespace, job) (
              rate(thanos_objstore_bucket_operation_failures_total{job=~"thanos-compact.*"}[5m])
            )
          /
            sum by (namespace, job) (rate(thanos_objstore_bucket_operations_total{job=~"thanos-compact.*"}[5m]))
        *
          100
      >
        5
    for: 15m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosCompactHasNotRun
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanoscompact
      description: Thanos Compact {{$labels.job}} in {{$labels.namespace}} has not
        uploaded anything for 24 hours.
      message: Thanos Compact {{$labels.job}} in {{$labels.namespace}} has not uploaded
        anything for 24 hours.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanoscompacthasnotrun
      summary: Thanos Compact has not uploaded anything for last 24 hours.
    expr: |2-
              time()
            -
              max by (namespace, job) (
                max_over_time(thanos_objstore_bucket_last_successful_upload_time{job=~"thanos-compact.*"}[1d])
              )
          /
            60
        /
          60
      >
        24
    for: 5m
    labels:
      service: thanos
      severity: medium
- name: thanos-query
  rules:
  - alert: ThanosQueryHttpRequestQueryErrorRateHigh
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosquery
      description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
        to handle {{$value | humanize}}% of "query" requests.
      message: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to
        handle {{$value | humanize}}% of "query" requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosqueryhttprequestqueryerrorratehigh
      summary: Thanos Query is failing to handle requests.
    expr: |2-
            sum by (namespace, job) (
              rate(http_requests_total{code=~"5..",handler="query",job=~"thanos-query.*"}[5m])
            )
          /
            sum by (namespace, job) (rate(http_requests_total{handler="query",job=~"thanos-query.*"}[5m]))
        *
          100
      >
        5
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosQueryGrpcServerErrorRate
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosquery
      description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
        to handle {{$value | humanize}}% of requests.
      message: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to
        handle {{$value | humanize}}% of requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosquerygrpcservererrorrate
      summary: Thanos Query is failing to handle requests.
    expr: |2-
            sum by (namespace, job) (
              rate(
                grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded",job=~"thanos-query.*"}[5m]
              )
            )
          /
            sum by (namespace, job) (rate(grpc_server_started_total{job=~"thanos-query.*"}[5m]))
        *
          100
      >
        5
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosQueryGrpcClientErrorRate
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosquery
      description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
        to send {{$value | humanize}}% of requests.
      message: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to
        send {{$value | humanize}}% of requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosquerygrpcclienterrorrate
      summary: Thanos Query is failing to send requests.
    expr: |2-
            sum by (namespace, job) (rate(grpc_client_handled_total{grpc_code!="OK",job=~"thanos-query.*"}[5m]))
          /
            sum by (namespace, job) (rate(grpc_client_started_total{job=~"thanos-query.*"}[5m]))
        *
          100
      >
        5
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosQueryHighDNSFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosquery
      description: Thanos Query {{$labels.job}} in {{$labels.namespace}} have {{$value
        | humanize}}% of failing DNS queries for store endpoints.
      message: Thanos Query {{$labels.job}} in {{$labels.namespace}} have {{$value
        | humanize}}% of failing DNS queries for store endpoints.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosqueryhighdnsfailures
      summary: Thanos Query is having high number of DNS failures.
    expr: |2-
            sum by (namespace, job) (
              rate(thanos_query_store_apis_dns_failures_total{job=~"thanos-query.*"}[5m])
            )
          /
            sum by (namespace, job) (rate(thanos_query_store_apis_dns_lookups_total{job=~"thanos-query.*"}[5m]))
        *
          100
      >
        1
    for: 15m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosQueryInstantLatencyHigh
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosquery
      description: Thanos Query {{$labels.job}} in {{$labels.namespace}} has a 99th
        percentile latency of {{$value}} seconds for instant queries.
      message: Thanos Query {{$labels.job}} in {{$labels.namespace}} has a 99th percentile
        latency of {{$value}} seconds for instant queries.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosqueryinstantlatencyhigh
      summary: Thanos Query has high latency for queries.
    expr: |2-
          histogram_quantile(
            0.99,
            sum by (namespace, job, le) (
              rate(http_request_duration_seconds_bucket{handler="query",job=~"thanos-query.*"}[5m])
            )
          )
        >
          90
      and
          sum by (namespace, job) (
            rate(http_request_duration_seconds_count{handler="query",job=~"thanos-query.*"}[5m])
          )
        >
          0
    for: 10m
    labels:
      service: thanos
      severity: high
- name: thanos-receive
  rules:
  - alert: ThanosReceiveHttpRequestErrorRateHigh
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to handle {{$value | humanize}}% of requests.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to handle {{$value | humanize}}% of requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivehttprequesterrorratehigh
      summary: Thanos Receive is failing to handle requests.
    expr: |2-
            sum by (namespace, job) (
              rate(http_requests_total{code=~"5..",handler="receive",job=~"thanos-receive-router.*"}[5m])
            )
          /
            sum by (namespace, job) (
              rate(http_requests_total{handler="receive",job=~"thanos-receive-router.*"}[5m])
            )
        *
          100
      >
        5
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosReceiveHttpRequestLatencyHigh
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} has a 99th
        percentile latency of {{ $value }} seconds for requests.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} has a 99th
        percentile latency of {{ $value }} seconds for requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivehttprequestlatencyhigh
      summary: Thanos Receive has high HTTP requests latency.
    expr: |2-
          histogram_quantile(
            0.99,
            sum by (namespace, job, le) (
              rate(http_request_duration_seconds_bucket{handler="receive",job=~"thanos-receive-router.*"}[5m])
            )
          )
        >
          10
      and
          sum by (namespace, job) (
            rate(http_request_duration_seconds_count{handler="receive",job=~"thanos-receive-router.*"}[5m])
          )
        >
          0
    for: 10m
    labels:
      service: thanos
      severity: high
  - alert: ThanosReceiveHighReplicationFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to replicate {{$value | humanize}}% of requests.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to replicate {{$value | humanize}}% of requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivehighreplicationfailures
      summary: Thanos Receive is having high number of replication failures.
    expr: |2-
        thanos_receive_replication_factor > 1
      and
              sum by (namespace, job) (
                rate(thanos_receive_replications_total{job=~"thanos-receive-router.*",result="error"}[5m])
              )
            /
              sum by (namespace, job) (
                rate(thanos_receive_replications_total{job=~"thanos-receive-router.*"}[5m])
              )
          >
              max by (namespace, job) (
                floor(thanos_receive_replication_factor{job=~"thanos-receive-router.*"} + 1 / 2)
              )
            /
              max by (namespace, job) (thanos_receive_hashring_nodes{job=~"thanos-receive-router.*"})
        *
          100
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosReceiveHighForwardRequestFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to forward {{$value | humanize}}% of requests.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to forward {{$value | humanize}}% of requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivehighforwardrequestfailures
      summary: Thanos Receive is failing to forward requests.
    expr: |2-
            sum by (namespace, job) (
              rate(thanos_receive_forward_requests_total{job=~"thanos-receive-router.*",result="error"}[5m])
            )
          /
            sum by (namespace, job) (
              rate(thanos_receive_forward_requests_total{job=~"thanos-receive-router.*"}[5m])
            )
        *
          100
      >
        20
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosReceiveHighHashringFileRefreshFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to refresh hashring file, {{$value | humanize}} of attempts failed.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        to refresh hashring file, {{$value | humanize}} of attempts failed.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivehighhashringfilerefreshfailures
      summary: Thanos Receive is failing to refresh hasring file.
    expr: |2-
          sum by (namespace, job) (
            rate(thanos_receive_hashrings_file_errors_total{job=~"thanos-receive-router.*"}[5m])
          )
        /
          sum by (namespace, job) (
            rate(thanos_receive_hashrings_file_refreshes_total{job=~"thanos-receive-router.*"}[5m])
          )
      >
        0
    for: 15m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosReceiveConfigReloadFailure
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} has not
        been able to reload hashring configurations.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} has not been
        able to reload hashring configurations.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceiveconfigreloadfailure
      summary: Thanos Receive is failing to reload hashring configurations.
    expr: |2-
        avg by (namespace, job) (
          thanos_receive_config_last_reload_successful{job=~"thanos-receive-router.*"}
        )
      !=
        1
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosReceiveNoUpload
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.instance}} in {{$labels.namespace}} has
        not uploaded latest data to object storage.
      message: Thanos Receive {{$labels.instance}} in {{$labels.namespace}} has not
        uploaded latest data to object storage.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivenoupload
      summary: Thanos Receive has not uploaded latest data to object storage.
    expr: |2-
        up{job=~"thanos-receive-ingester.*"} - 1
      + on (namespace, job, instance)
          sum by (namespace, job, instance) (
            increase(thanos_shipper_uploads_total{job=~"thanos-receive-ingester.*"}[3h])
          )
        ==
          0
    for: 3h
    labels:
      service: thanos
      severity: high
  - alert: ThanosReceiveLimitsConfigReloadFailure
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} has not
        been able to reload the limits configuration.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} has not been
        able to reload the limits configuration.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivelimitsconfigreloadfailure
      summary: Thanos Receive has not been able to reload the limits configuration.
    expr: |2-
        sum by (namespace, job) (
          increase(thanos_receive_limits_config_reload_err_total{job=~"thanos-receive-router.*"}[5m])
        )
      >
        0
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosReceiveLimitsHighMetaMonitoringQueriesFailureRate
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        for {{$value | humanize}}% of meta monitoring queries.
      message: Thanos Receive {{$labels.job}} in {{$labels.namespace}} is failing
        for {{$value | humanize}}% of meta monitoring queries.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivelimitshighmetamonitoringqueriesfailurerate
      summary: Thanos Receive has not been able to update the number of head series.
    expr: |2-
            sum by (namespace, job) (
              increase(thanos_receive_metamonitoring_failed_queries_total{job=~"thanos-receive-router.*"}[5m])
            )
          /
            20
        *
          100
      >
        20
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosReceiveTenantLimitedByHeadSeries
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosreceive
      description: Thanos Receive tenant {{$labels.tenant}} in {{$labels.namespace}}
        is limited by head series.
      message: Thanos Receive tenant {{$labels.tenant}} in {{$labels.namespace}} is
        limited by head series.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosreceivetenantlimitedbyheadseries
      summary: Thanos Receive tenant is limited by head series.
    expr: |2-
        sum by (namespace, job, tenant) (
          increase(thanos_receive_head_series_limited_requests_total{job=~"thanos-receive-router.*"}[5m])
        )
      >
        0
    for: 5m
    labels:
      service: thanos
      severity: medium
- name: thanos-store
  rules:
  - alert: ThanosStoreGrpcErrorRate
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosstore
      description: Thanos Store {{$labels.job}} in {{$labels.namespace}} is failing
        to handle {{$value | humanize}}% of requests.
      message: Thanos Store {{$labels.job}} in {{$labels.namespace}} is failing to
        handle {{$value | humanize}}% of requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosstoregrpcerrorrate
      summary: Thanos Store is failing to handle gRPC requests.
    expr: |2-
            sum by (namespace, job) (
              rate(
                grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded",job=~"thanos-store.*"}[5m]
              )
            )
          /
            sum by (namespace, job) (rate(grpc_server_started_total{job=~"thanos-store.*"}[5m]))
        *
          100
      >
        5
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosStoreBucketHighOperationFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosstore
      description: Thanos Store {{$labels.job}} in {{$labels.namespace}} Bucket is
        failing to execute {{$value | humanize}}% of operations.
      message: Thanos Store {{$labels.job}} in {{$labels.namespace}} Bucket is failing
        to execute {{$value | humanize}}% of operations.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosstorebuckethighoperationfailures
      summary: Thanos Store Bucket is failing to execute operations.
    expr: |2-
            sum by (namespace, job) (
              rate(thanos_objstore_bucket_operation_failures_total{job=~"thanos-store.*"}[5m])
            )
          /
            sum by (namespace, job) (rate(thanos_objstore_bucket_operations_total{job=~"thanos-store.*"}[5m]))
        *
          100
      >
        5
    for: 15m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosStoreObjstoreOperationLatencyHigh
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosstore
      description: Thanos Store {{$labels.job}} in {{$labels.namespace}} Bucket has
        a 99th percentile latency of {{$value}} seconds for the bucket operations.
      message: Thanos Store {{$labels.job}} in {{$labels.namespace}} Bucket has a
        99th percentile latency of {{$value}} seconds for the bucket operations.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosstoreobjstoreoperationlatencyhigh
      summary: Thanos Store is having high latency for bucket operations.
    expr: |2-
          histogram_quantile(
            0.99,
            sum by (namespace, job, le) (
              rate(thanos_objstore_bucket_operation_duration_seconds_bucket{job=~"thanos-store.*"}[5m])
            )
          )
        >
          7
      and
          sum by (namespace, job) (
            rate(thanos_objstore_bucket_operation_duration_seconds_count{job=~"thanos-store.*"}[5m])
          )
        >
          0
    for: 10m
    labels:
      service: thanos
      severity: medium
- name: thanos-rule
  rules:
  - alert: ThanosRuleQueueIsDroppingAlerts
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
        to queue rulehelpers.
      message: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
        to queue rulehelpers.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulequeueisdroppingalerts
      summary: Thanos Rule is failing to queue rulehelpers.
    expr: |2-
        sum by (namespace, job, instance) (
          rate(thanos_alert_queue_alerts_dropped_total{job=~"thanos-ruler.*"}[5m])
        )
      >
        0
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosRuleSenderIsFailingAlerts
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
        to send alerts to alertmanager.
      message: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
        to send alerts to alertmanager.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulesenderisfailingalerts
      summary: Thanos Rule is failing to send alerts to alertmanager.
    expr: |2-
        sum by (namespace, job, instance) (
          rate(thanos_alert_sender_alerts_dropped_total{job=~"thanos-ruler.*"}[5m])
        )
      >
        0
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosRuleHighRuleEvaluationFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
        to evaluate rules.
      message: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
        to evaluate rules.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulehighruleevaluationfailures
      summary: Thanos Rule is failing to evaluate rules.
    expr: |2-
            sum by (namespace, job, instance) (
              rate(prometheus_rule_evaluation_failures_total{job=~"thanos-ruler.*"}[5m])
            )
          /
            sum by (namespace, job, instance) (
              rate(prometheus_rule_evaluations_total{job=~"thanos-ruler.*"}[5m])
            )
        *
          100
      >
        5
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosRuleHighRuleEvaluationWarnings
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has high
        number of evaluation warnings.
      message: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has high
        number of evaluation warnings.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulehighruleevaluationwarnings
      summary: Thanos Rule has high number of evaluation warnings.
    expr: |2-
        sum by (namespace, job, instance) (
          rate(thanos_rule_evaluation_with_warnings_total{job=~"thanos-ruler.*"}[5m])
        )
      >
        0
    for: 15m
    labels:
      service: thanos
      severity: high
  - alert: ThanosRuleRuleEvaluationLatencyHigh
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has higher
        evaluation latency than interval for {{$labels.rule_group}}.
      message: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has higher
        evaluation latency than interval for {{$labels.rule_group}}.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosruleruleevaluationlatencyhigh
      summary: Thanos Rule has high rule evaluation latency.
    expr: |2-
        sum by (namespace, job, instance, rule_group) (
          prometheus_rule_group_last_duration_seconds{job=~"thanos-ruler.*"}
        )
      >
        sum by (namespace, job, instance, rule_group) (
          prometheus_rule_group_interval_seconds{job=~"thanos-ruler.*"}
        )
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosRuleGrpcErrorRate
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing
        to handle {{$value | humanize}}% of requests.
      message: Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing to
        handle {{$value | humanize}}% of requests.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulegrpcerrorrate
      summary: Thanos Rule is failing to handle grpc requests.
    expr: |2-
            sum by (namespace, job, instance) (
              rate(
                grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded",job=~"thanos-ruler.*"}[5m]
              )
            )
          /
            sum by (namespace, job, instance) (rate(grpc_server_started_total{job=~"thanos-ruler.*"}[5m]))
        *
          100
      >
        5
    for: 5m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosRuleConfigReloadFailure
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not been
        able to reload its configuration.
      message: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not been able
        to reload its configuration.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosruleconfigreloadfailure
      summary: Thanos Rule has not been able to reload configuration.
    expr: |2-
        avg by (namespace, job, instance) (thanos_rule_config_last_reload_successful{job=~"thanos-ruler.*"})
      !=
        1
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosRuleQueryHighDNSFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value
        | humanize}}% of failing DNS queries for query endpoints.
      message: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value |
        humanize}}% of failing DNS queries for query endpoints.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulequeryhighdnsfailures
      summary: Thanos Rule is having high number of DNS failures.
    expr: |2-
            sum by (namespace, job, instance) (
              rate(thanos_rule_query_apis_dns_failures_total{job=~"thanos-ruler.*"}[5m])
            )
          /
            sum by (namespace, job, instance) (
              rate(thanos_rule_query_apis_dns_lookups_total{job=~"thanos-ruler.*"}[5m])
            )
        *
          100
      >
        1
    for: 15m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosRuleAlertmanagerHighDNSFailures
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has {{$value
        | humanize}}% of failing DNS queries for Alertmanager endpoints.
      message: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has {{$value
        | humanize}}% of failing DNS queries for Alertmanager endpoints.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulealertmanagerhighdnsfailures
      summary: Thanos Rule is having high number of DNS failures.
    expr: |2-
            sum by (namespace, job, instance) (
              rate(thanos_rule_alertmanagers_dns_failures_total{job=~"thanos-ruler.*"}[5m])
            )
          /
            sum by (namespace, job, instance) (
              rate(thanos_rule_alertmanagers_dns_lookups_total{job=~"thanos-ruler.*"}[5m])
            )
        *
          100
      >
        1
    for: 15m
    labels:
      service: thanos
      severity: medium
  - alert: ThanosRuleNoEvaluationFor10Intervals
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has rule groups
        that did not evaluate for at least 10x of their expected interval.
      message: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has rule groups
        that did not evaluate for at least 10x of their expected interval.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosrulenoevaluationfor10intervals
      summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
    expr: |2-
          time()
        -
          max by (namespace, job, instance, group) (
            prometheus_rule_group_last_evaluation_timestamp_seconds{job=~"thanos-ruler.*"}
          )
      >
          10
        *
          max by (namespace, job, instance, group) (
            prometheus_rule_group_interval_seconds{job=~"thanos-ruler.*"}
          )
    for: 5m
    labels:
      service: thanos
      severity: high
  - alert: ThanosNoRuleEvaluations
    annotations:
      dashboard: https://demo.perses.dev/projects/perses/dashboards/thanosrule
      description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did not
        perform any rule evaluations in the past 10 minutes.
      message: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did not perform
        any rule evaluations in the past 10 minutes.
      runbook: https://github.com/thanos-io/thanos/blob/main/mixin/runbook.md#thanosnoruleevaluations
      summary: Thanos Rule did not perform any rule evaluations.
    expr: |2-
          sum by (namespace, job, instance) (
            rate(prometheus_rule_evaluations_total{job=~"thanos-ruler.*"}[5m])
          )
        <=
          0
      and
        sum by (namespace, job, instance) (thanos_rule_loaded_rules{job=~"thanos-ruler.*"}) > 0
    for: 5m
    labels:
      service: thanos
      severity: critical
